{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ecee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded for environment: dev\n",
      "Data loaded from ../data/data.csv, shape: (36275, 19)\n",
      "Starting data preprocessing...\n",
      "Preprocessing complete. DataFrame shape: (36275, 27)\n",
      "Numeric features: ['no_of_adults', 'no_of_children', 'no_of_weekend_nights', 'no_of_week_nights', 'required_car_parking_space', 'lead_time', 'arrival_year', 'arrival_month', 'arrival_date', 'repeated_guest', 'no_of_previous_cancellations', 'no_of_previous_bookings_not_canceled', 'avg_price_per_room', 'no_of_special_requests', 'total_nights', 'total_guests', 'price_per_person', 'price_per_night', 'with_children', 'has_weekend_stay']\n",
      "Categorical features: ['type_of_meal_plan', 'room_type_reserved', 'market_segment_type', 'booking_season']\n",
      "Splitting data into train and test sets...\n",
      "Data split complete. Train set: (29020, 27), Test set: (7255, 27)\n",
      "\u001b[32m2025-05-21 11:19:38\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36m865111206\u001b[0m:\u001b[36mmain\u001b[0m - \u001b[1mTraining set shape: %s\u001b[0m\n",
      "\u001b[32m2025-05-21 11:19:38\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36m865111206\u001b[0m:\u001b[36mmain\u001b[0m - \u001b[1mTest set shape: %s\u001b[0m\n",
      "Saving data to Databricks catalog...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\Desktop\\marvelous_mlops_course\\marvelous-databricks-course-DylanAustin\\src\\hotel_reservations\\data_preprocessor.py:91: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.df['no_of_children'].fillna(0, inplace=True)\n",
      "C:\\Users\\dylan\\Desktop\\marvelous_mlops_course\\marvelous-databricks-course-DylanAustin\\src\\hotel_reservations\\data_preprocessor.py:92: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.df['no_of_previous_cancellations'].fillna(0, inplace=True)\n",
      "C:\\Users\\dylan\\Desktop\\marvelous_mlops_course\\marvelous-databricks-course-DylanAustin\\src\\hotel_reservations\\data_preprocessor.py:93: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.df['no_of_previous_bookings_not_canceled'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to tables: mlops_dev.dylanaus.train_set and mlops_dev.dylanaus.test_set\n",
      "Enabling Change Data Feed...\n",
      "Change Data Feed enabled for train and test set tables\n",
      "Processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is an example of a Python script that processes hotel reservation data using src/hotel_reservations/data_preprocessor.py.\n",
    "This was developed for Deliverable 1 of the MarvelousMLOps project.\n",
    "'''\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from hotel_reservations.config import ProjectConfig\n",
    "from hotel_reservations.data_preprocessor import DataProcessor\n",
    "from marvelous.logging import setup_logging\n",
    "from marvelous.timer import Timer\n",
    "\n",
    "def main(config_path: str, data_path: str = \"../data/data.csv\", env: str = \"dev\") -> None:\n",
    "    \"\"\"Main function to process hotel cancellation data.\n",
    "    \n",
    "    :param config_path: Path to the YAML configuration file\n",
    "    :param env: Environment to use (dev, acc, prd)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Establish config\n",
    "    config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\", env=\"dev\")\n",
    "\n",
    "    # Set up logging\n",
    "    setup_logging(log_file=\"logs/marvelous-1da.log\") # Assuming this is required and works.\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Load and validate the configuration\n",
    "    try:\n",
    "        config = ProjectConfig.from_yaml(config_path, env)\n",
    "        print(f\"Configuration loaded for environment: {env}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading configuration: {e}\")\n",
    "        spark.stop()\n",
    "        return\n",
    "    \n",
    "    # Load the data\n",
    "    try:\n",
    "        raw_data = pd.read_csv(data_path)\n",
    "        print(f\"Data loaded from {data_path}, shape: {raw_data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        spark.stop()\n",
    "        return\n",
    "\n",
    "    # Initialize the DataPreprocessor class that handles the data processing\n",
    "    processor = DataProcessor(raw_data, config, spark)\n",
    "\n",
    "    # Preprocess the data\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    processor.preprocess()\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    print(\"Splitting data into train and test sets...\")\n",
    "    train_set, test_set = processor.split_data()\n",
    "    logger.info(\"Training set shape: %s\", train_set.shape)\n",
    "    logger.info(\"Test set shape: %s\", test_set.shape)\n",
    "    \n",
    "    # Save the data to Databricks tables\n",
    "    print(\"Saving data to Databricks catalog...\")\n",
    "    processor.save_to_catalog(train_set, test_set)\n",
    "    \n",
    "    # Enable change data feed\n",
    "    print(\"Enabling Change Data Feed...\")\n",
    "    processor.enable_change_data_feed()\n",
    "    \n",
    "    print(\"Processing completed successfully!\")\n",
    "    \n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "\n",
    "config_path = \"../project_config.yml\"\n",
    "data_path = \"../data/data.csv\"\n",
    "env = \"dev\"\n",
    "\n",
    "main(config_path=config_path, data_path=data_path, env=env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
